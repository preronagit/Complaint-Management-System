{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preronagit/Complaint-Management-System/blob/main/Complaint_Management_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "latest code\n"
      ],
      "metadata": {
        "id": "kOOTqXJ4JNZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase, remove stopwords\n",
        "    if isinstance(text, str):\n",
        "        return ' '.join([word for word in text.lower().split() if word not in stop_words])\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "df['cleaned_complaint'] = df['Complaint Description'].apply(preprocess)\n",
        "\n",
        "df.dropna(subset=['Category'], inplace=True)\n",
        "\n",
        "# Split the dataset\n",
        "X = df['cleaned_complaint']\n",
        "y = df['Category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Load pre-trained DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def get_distilbert_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Get embeddings for all complaints\n",
        "X_train_embeddings = np.vstack([get_distilbert_embeddings(text) for text in X_train])\n",
        "X_test_embeddings = np.vstack([get_distilbert_embeddings(text) for text in X_test])\n",
        "\n",
        "# Dimensionality reduction with PCA\n",
        "pca = PCA(n_components=50)  # Adjust to find the best balance of dimensionality and performance\n",
        "X_train_pca = pca.fit_transform(X_train_embeddings)\n",
        "X_test_pca = pca.transform(X_test_embeddings)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbc.fit(X_train_pca, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = gbc.predict(X_test_pca)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred)*100, \"%\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Function to classify new complaints\n",
        "def classify_complaint(complaint, model, pca):\n",
        "    complaint_cleaned = preprocess(complaint)\n",
        "    complaint_embedding = get_distilbert_embeddings(complaint_cleaned)\n",
        "    complaint_pca = pca.transform(complaint_embedding)\n",
        "    return model.predict(complaint_pca)[0]\n",
        "\n",
        "# Function to get category recommendations based on input\n",
        "def recommend_categories(input_text, n_recommendations=3):\n",
        "    input_cleaned = preprocess(input_text)\n",
        "    input_embedding = get_distilbert_embeddings(input_cleaned)\n",
        "    input_pca = pca.transform(input_embedding)\n",
        "\n",
        "    # Calculate cosine similarity with training data\n",
        "    similarities = cosine_similarity(input_pca, X_train_pca)\n",
        "    similar_indices = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    recommended_categories = []\n",
        "    for index in similar_indices[:n_recommendations]:\n",
        "        recommended_categories.append(y_train.iloc[index])\n",
        "\n",
        "    return list(set(recommended_categories))\n",
        "\n",
        "# Main loop for user input\n",
        "def main():\n",
        "    print(\"Welcome to the Complaint Categorization System!\")\n",
        "    print(\"Type your complaint below (or type 'exit' to quit):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Complaint: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Exiting the system. Thank you!\")\n",
        "            break\n",
        "\n",
        "        # Recommend categories\n",
        "        recommended_categories = recommend_categories(user_input)\n",
        "        print(\"Recommended Categories:\", recommended_categories)\n",
        "\n",
        "        # Classify using the best model\n",
        "        predicted_category = classify_complaint(user_input, gbc, pca)\n",
        "        print(\"Predicted Category:\", predicted_category)\n",
        "        print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Hnxnk9SdJQ2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}